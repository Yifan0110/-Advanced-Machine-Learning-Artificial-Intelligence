{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffbbd8b8-b700-4903-ac4e-032cd44877f8",
   "metadata": {},
   "source": [
    "# Assignment 2, Yifan Han, Oct 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7106be-65cb-4a36-af36-912ec6795209",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63edd8-b023-4fb2-a436-9800bc0647be",
   "metadata": {},
   "source": [
    "2. Presumably you know well at least one more language besides English (if not, you have a week to learn). Use pipelines() from jupyter notebook to translate a page of some interesting text T from language L1 to L2. How good is the translation?\n",
    "\n",
    "It looks good, some words couldn't be translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5baf2b8-95b4-4028-9b1d-820d92ab4855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Define the Chinese to English translation pipeline\n",
    "zh_to_en_translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-zh-en\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5df5ef44-82ae-4ead-a73e-0561b3add264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The groom hides his wedding ring in a potato box and delivers it to the bride from the “twice to get it” window; there are potatoes and tomatoes on the scene, McDonald's logo as the background wall of the wedding dress; a dozen friends are invited to celebrate, and the guest meal is a 98-dollar children's package. The idea of a wedding in McDonald's coincides with the husband's family. The bride's best friend is responsible for the wedding, and the two parties have a white line, unlike the traditional wedding vows, and the two are present and comfortable.\n"
     ]
    }
   ],
   "source": [
    "# Example Chinese text\n",
    "text_zh = \"\"\"\n",
    "新郎把婚戒藏在薯条盒里，从“得来速”窗口递给新娘；现场布满土豆和西红柿，麦当劳标识作为婚纱照背景墙；邀请十来个好友庆祝，嘉宾餐食是98元儿童套餐。在麦当劳举办婚礼的想法，杨甜甜和丈夫李家胜一拍即合。两人经常去吃麦当劳，家里还收藏了很多小玩具，“给长辈一个‘庄重的传统婚礼’，何不给自己一个轻松快乐的仪式？”两人最终选择了汉阳区一家双层独栋麦当劳。工作人员得知要办婚礼，还特意换上了节日才穿的粉色制服。新娘的好朋友负责主持婚礼，双方有一个互相告白的环节，不同于传统婚礼上庄重的誓言，两人临场发挥，轻松又温馨。\n",
    "\"\"\"\n",
    "\n",
    "# Translate from Chinese to English\n",
    "translated_text_en = zh_to_en_translator(text_zh, max_length=512)\n",
    "print(translated_text_en[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7875c04a-2ef4-44c6-9afa-95fa66694ccf",
   "metadata": {},
   "source": [
    "3. Now take the translation in L2 and translate it back to L1. How different is this translation from the original text? How good is the reconstructed text?\n",
    "\n",
    "It's totally different from the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0f8977e7-c21f-44dc-9bf1-ae6dacd4e13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新郎将结婚戒指藏在一个土豆盒里,并将戒指从“两次买下婚戒”窗口交给新娘;现场有土豆和番茄,麦当劳的标志是婚纱的背景墙;十几个朋友被邀请庆祝,客餐是98美元孩子的包裹。麦当劳的婚礼想法与丈夫的家庭相吻合。新娘最好的朋友负责婚礼,双方有一条白线,与传统的婚礼誓言不同,两人都在场,舒适。\n"
     ]
    }
   ],
   "source": [
    "# Define the English to Chinese translation pipeline\n",
    "en_to_zh_translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\", device='cuda')\n",
    "\n",
    "# Translate from English back to Chinese\n",
    "reconstructed_text_zh = en_to_zh_translator(translated_text_en[0]['translation_text'], max_length=512)\n",
    "print(reconstructed_text_zh[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a14c58-9e5c-44e8-aa86-fe544ec4f087",
   "metadata": {},
   "source": [
    "4. Start with T in L1 and let pipelines() generate the second page. Does the generated text make sense? Do the same with T in L2.\n",
    "\n",
    "For the English generation, it makes sense. But for the Chinese one, it doesn't make sense at all, and I can't even read some characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "844cb0fe-9ee4-4793-a37e-f0c159c87b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The groom hides his wedding ring in a potato box and delivers it to the bride from the “twice to get it” window; there are potatoes and tomatoes on the scene, McDonald's logo as the background wall of the wedding dress; a dozen friends are invited to celebrate, and the guest meal is a 98-dollar children's package. The idea of a wedding in McDonald's coincides with the husband's family. The bride's best friend is responsible for the wedding, and the two parties have a white line, unlike the traditional wedding vows, and the two are present and comfortable.\n",
      "\n",
      "According to the Washington Post, the husband had decided to invite his wife into a \"private, family-owned\" house for food and to have one of their friends do the cooking. While there, the groom took on a job as a barber; for some reason he didn't bring anything to eat, or to help out, the Post wrote. \"The first time I saw him,\" the Post's James Fierstein concluded, \"I couldn't have asked for a better family friend and caretaker or for the groom to be such a friend.\"\n",
      "\n",
      "The bride-to-be and her wedding day's food was an excellent example of this:\n",
      "\n",
      "The groom did eat a few times, but his food was mostly vegetables with vegetables. So I took him home and his vegetables were a bit thick and he had to eat the vegetables with the potatoes.\n",
      "\n",
      "At a dinner party on the evening of the bride's wedding, some friends with families (including this couple, for one) had brought the bride some pasta. Afterward, one of the guests noticed that it was only a couple of minutes before the dress, and his guests asked for his blessing.\n",
      "\n",
      "For the bride's wedding night, McDonald's serves free sandwiches and desserts to all guests. On the afternoon of the groom's return to the kitchen, the restaurant is open 24:25 with a wait to take pictures.\n",
      "\n",
      "But for a couple of hours after the wedding, the groom was out and at his new job working as kitchen assistant. At first, he didn't have that many minutes and he had to run the kitchen back and forth across the kitchen, the Post noted. He was eventually asked for advice about the next day's date after lunch (some of it was for food and other stuff that he'd been asked for) and he didn't know how to come up with the cost of the time. His manager recommended he get out\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation',device='cuda', pad_token_id=50256)\n",
    "\n",
    "# Generate continuation in English\n",
    "generated_text_en = generator(translated_text_en[0]['translation_text'], max_length=512)\n",
    "print(generated_text_en[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "95beeb2d-aa57-427a-a13a-9c779ac465da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新郎将结婚戒指藏在一个土豆盒里,并将戒指从“两次买下婚戒”窗口交给新娘;现场有土豆和番茄,麦当劳的标志是婚纱的背景墙;十几个朋友被邀请庆祝,客餐是98美元孩子的包裹。麦当劳的婚礼想法与丈夫的家庭相吻合。新娘最好的朋友负责婚礼,双方有一条白线,与传统的婚礼誓言不同,两人都在场,舒适。这种架圾可有的异吁逼上,这坊城在开始国大方法和置种近。六于及有阿是塞鳴的有助发贴得币重一许。在欴所上海又自由都伝吰求公又尋。\n",
      "\n",
      "不是在建得必队的普,这就传会一和和偁,求深等到在跳政凍。我许的桰在眼礼景线一样找许的事,如\n"
     ]
    }
   ],
   "source": [
    "# Generate continuation in Chinese\n",
    "generated_text_zh = generator(reconstructed_text_zh[0]['translation_text'], max_length=512)\n",
    "print(generated_text_zh[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe41c23-abea-4e45-b791-2855424ee5fc",
   "metadata": {},
   "source": [
    "5. Use pipelines() to summarize T in L1 and T in L2. Does each summary make sense? Are they similar?\n",
    "\n",
    "Overall the summarization is right but the Englished one added NYC for no reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b89b9586-78d6-43bb-9bbe-c552b22e857a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "农历新年伊始,中国湖南汉阳区一家双层独栋麦当劳举办了传统的婚礼。\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"csebuetnlp/mT5_multilingual_XLsum\", device='cuda')\n",
    "summary_zh = summarizer(text_zh, max_length=150)\n",
    "print(summary_zh[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d0a1c7c0-5fbe-4c6b-91f6-510f01ae943a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wedding in McDonald's has taken place in the US city of New York.\n"
     ]
    }
   ],
   "source": [
    "# Summarize English translation\n",
    "summary_en = summarizer(translated_text_en[0]['translation_text'], max_length=150)\n",
    "print(summary_en[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f00bc0-2198-475c-8bb0-c72ef55d8691",
   "metadata": {},
   "source": [
    "6. Use pipelines() to ask questions about T in L1 and same questions about T in L2. Do the answers make sense? Are they similar between the languages?\n",
    "\n",
    "The Chinese answer is totally wrong and unrelated but the English one is right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "29b8b7fd-27ac-4c1c-8781-38720ed7461c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "郎\n"
     ]
    }
   ],
   "source": [
    "qa = pipeline(\"question-answering\", device= 'cuda')\n",
    "\n",
    "# Example question in Chinese\n",
    "qa_zh = qa(question=\"戒指放在哪里？\", context=text_zh)\n",
    "print(qa_zh['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3e85083d-fca9-46cb-9f6a-69c09202ad72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McDonald's\n"
     ]
    }
   ],
   "source": [
    "# Example question in English\n",
    "qa_en = qa(question=\"Where was the wedding\", context=translated_text_en[0]['translation_text'])\n",
    "print(qa_en['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042649b9-8db8-429c-abc7-61e3d2157f77",
   "metadata": {},
   "source": [
    "7. Run NER on T in L1 and T in L2. Does it look correct?\n",
    "\n",
    "English one looks correct while the Chinese one doen't look correct maybe because of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7d11b673-7a28-4186-aeda-02c902ac3cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-company', 'score': 0.6130998, 'index': 38, 'word': '当', 'start': 38, 'end': 39}, {'entity': 'I-company', 'score': 0.54386395, 'index': 39, 'word': '劳', 'start': 39, 'end': 40}, {'entity': 'B-address', 'score': 0.52668583, 'index': 74, 'word': '麦', 'start': 75, 'end': 76}, {'entity': 'B-name', 'score': 0.98735934, 'index': 85, 'word': '杨', 'start': 86, 'end': 87}, {'entity': 'I-name', 'score': 0.9159081, 'index': 86, 'word': '甜', 'start': 87, 'end': 88}, {'entity': 'I-name', 'score': 0.93037415, 'index': 87, 'word': '甜', 'start': 88, 'end': 89}, {'entity': 'B-name', 'score': 0.98933697, 'index': 91, 'word': '李', 'start': 92, 'end': 93}, {'entity': 'I-name', 'score': 0.9804458, 'index': 92, 'word': '家', 'start': 93, 'end': 94}, {'entity': 'I-name', 'score': 0.9833618, 'index': 93, 'word': '胜', 'start': 94, 'end': 95}, {'entity': 'B-address', 'score': 0.96737444, 'index': 160, 'word': '汉', 'start': 161, 'end': 162}, {'entity': 'I-address', 'score': 0.9277305, 'index': 161, 'word': '阳', 'start': 162, 'end': 163}, {'entity': 'I-address', 'score': 0.92148775, 'index': 162, 'word': '区', 'start': 163, 'end': 164}]\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", model=\"uer/roberta-base-finetuned-cluener2020-chinese\", tokenizer=\"uer/roberta-base-finetuned-cluener2020-chinese\", device='cuda')\n",
    "\n",
    "# NER in Chinese\n",
    "entities_zh = ner(text_zh)\n",
    "print(entities_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4e9d9655-9f43-4cfc-aacf-a280cecded34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'ORG', 'score': 0.9974422, 'word': \"McDonald ' s\", 'start': 159, 'end': 169}, {'entity_group': 'ORG', 'score': 0.9468536, 'word': \"McDonald ' s\", 'start': 341, 'end': 351}]\n"
     ]
    }
   ],
   "source": [
    "# NER in English\n",
    "ner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", device=0, aggregation_strategy=\"simple\")\n",
    "\n",
    "# NER in English\n",
    "entities_en = ner(translated_text_en[0]['translation_text'])\n",
    "print(entities_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40caf5a3-350c-462e-8f1a-1eeb5a4def4e",
   "metadata": {},
   "source": [
    "8. Run sentiment analysis on T in L1 and T in L2. Does it look correct?\n",
    "\n",
    "Basically it looks correct because both are positive. But the score is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d146bb52-2b0c-4ac3-a465-cbc8c1e9bea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '5 stars', 'score': 0.7358742952346802}]\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\", device='cuda')\n",
    "\n",
    "# Sentiment in Chinese\n",
    "sentiment_zh = sentiment_analyzer(text_zh)\n",
    "print(sentiment_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "adc622c5-5a2e-4455-8eb6-f97b2bbe7ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '4 stars', 'score': 0.5395357608795166}]\n"
     ]
    }
   ],
   "source": [
    "# Sentiment in English\n",
    "sentiment_en = sentiment_analyzer(translated_text_en[0]['translation_text'])\n",
    "print(sentiment_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99458e17-648e-4b8d-a083-4a8b8902e585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
