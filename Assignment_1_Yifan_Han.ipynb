{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e13e4b0-234b-45b9-925d-306b8d33e1b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment 1, Yifan Han, Oct 17 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0751a-7b74-4f30-b859-0ddd482ea06c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e8cdabd-aaff-4072-9e30-f08dc38195bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import time\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = NeuralNetwork().to(device)  # Move the model to the GPU/CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7256d-e67c-45cd-9690-f92f4380acc3",
   "metadata": {},
   "source": [
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  Tesla T4                       On  |   00000000:00:04.0 Off |                    0 |\n",
    "| N/A   46C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "                                                                                         \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|  No running processes found                                                             |\n",
    "+-----------------------------------------------------------------------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4646e0-c974-4684-bb08-2b8f8833b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GPU is an NVIDIA Tesla T4. The memory of the GPU is 15,360 MiB (15 GB), as seen in the \"Memory-Usage\" section.\n"
     ]
    }
   ],
   "source": [
    "# Which GPU you used? How much memory does it have?\n",
    "\n",
    "print('The GPU is an NVIDIA Tesla T4. The memory of the GPU is 15,360 MiB (15 GB), as seen in the \"Memory-Usage\" section.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d27bad-f432-4b93-866d-7c60ab0ab284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Confirm inside jupyter or python that your PyTorch installation supports GPU.\n",
    "print(torch.cuda.is_available())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "653af085-d286-472f-9b0f-2adbad3626fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dadae4e-aa2c-4f8e-a376-5184fcaf7c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.308072  [   64/60000]\n",
      "loss: 2.290076  [ 6464/60000]\n",
      "loss: 2.271990  [12864/60000]\n",
      "loss: 2.268460  [19264/60000]\n",
      "loss: 2.243478  [25664/60000]\n",
      "loss: 2.221152  [32064/60000]\n",
      "loss: 2.224496  [38464/60000]\n",
      "loss: 2.192322  [44864/60000]\n",
      "loss: 2.193143  [51264/60000]\n",
      "loss: 2.159572  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.5%, Avg loss: 2.153347 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.168822  [   64/60000]\n",
      "loss: 2.156839  [ 6464/60000]\n",
      "loss: 2.094479  [12864/60000]\n",
      "loss: 2.117905  [19264/60000]\n",
      "loss: 2.053223  [25664/60000]\n",
      "loss: 1.998184  [32064/60000]\n",
      "loss: 2.026289  [38464/60000]\n",
      "loss: 1.944067  [44864/60000]\n",
      "loss: 1.952070  [51264/60000]\n",
      "loss: 1.877596  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss: 1.874624 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.915162  [   64/60000]\n",
      "loss: 1.884786  [ 6464/60000]\n",
      "loss: 1.754298  [12864/60000]\n",
      "loss: 1.807019  [19264/60000]\n",
      "loss: 1.685389  [25664/60000]\n",
      "loss: 1.635760  [32064/60000]\n",
      "loss: 1.658327  [38464/60000]\n",
      "loss: 1.554096  [44864/60000]\n",
      "loss: 1.579494  [51264/60000]\n",
      "loss: 1.477696  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 1.497774 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.570135  [   64/60000]\n",
      "loss: 1.537866  [ 6464/60000]\n",
      "loss: 1.377551  [12864/60000]\n",
      "loss: 1.461797  [19264/60000]\n",
      "loss: 1.333608  [25664/60000]\n",
      "loss: 1.329843  [32064/60000]\n",
      "loss: 1.337738  [38464/60000]\n",
      "loss: 1.264244  [44864/60000]\n",
      "loss: 1.299693  [51264/60000]\n",
      "loss: 1.208785  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.237907 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.319218  [   64/60000]\n",
      "loss: 1.301557  [ 6464/60000]\n",
      "loss: 1.132019  [12864/60000]\n",
      "loss: 1.246252  [19264/60000]\n",
      "loss: 1.112092  [25664/60000]\n",
      "loss: 1.140865  [32064/60000]\n",
      "loss: 1.151487  [38464/60000]\n",
      "loss: 1.094445  [44864/60000]\n",
      "loss: 1.133013  [51264/60000]\n",
      "loss: 1.061623  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.082808 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.158423  [   64/60000]\n",
      "loss: 1.158280  [ 6464/60000]\n",
      "loss: 0.974519  [12864/60000]\n",
      "loss: 1.114964  [19264/60000]\n",
      "loss: 0.976944  [25664/60000]\n",
      "loss: 1.016388  [32064/60000]\n",
      "loss: 1.040213  [38464/60000]\n",
      "loss: 0.988966  [44864/60000]\n",
      "loss: 1.024439  [51264/60000]\n",
      "loss: 0.971381  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.982971 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.046368  [   64/60000]\n",
      "loss: 1.066261  [ 6464/60000]\n",
      "loss: 0.866711  [12864/60000]\n",
      "loss: 1.027049  [19264/60000]\n",
      "loss: 0.890446  [25664/60000]\n",
      "loss: 0.927448  [32064/60000]\n",
      "loss: 0.967290  [38464/60000]\n",
      "loss: 0.920322  [44864/60000]\n",
      "loss: 0.947089  [51264/60000]\n",
      "loss: 0.909518  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.913427 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.961972  [   64/60000]\n",
      "loss: 1.000903  [ 6464/60000]\n",
      "loss: 0.787902  [12864/60000]\n",
      "loss: 0.962961  [19264/60000]\n",
      "loss: 0.831174  [25664/60000]\n",
      "loss: 0.860100  [32064/60000]\n",
      "loss: 0.914815  [38464/60000]\n",
      "loss: 0.874189  [44864/60000]\n",
      "loss: 0.889230  [51264/60000]\n",
      "loss: 0.863151  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.861890 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.895370  [   64/60000]\n",
      "loss: 0.950463  [ 6464/60000]\n",
      "loss: 0.727485  [12864/60000]\n",
      "loss: 0.913466  [19264/60000]\n",
      "loss: 0.787959  [25664/60000]\n",
      "loss: 0.807596  [32064/60000]\n",
      "loss: 0.874240  [38464/60000]\n",
      "loss: 0.841753  [44864/60000]\n",
      "loss: 0.844764  [51264/60000]\n",
      "loss: 0.826211  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.821854 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.840829  [   64/60000]\n",
      "loss: 0.908950  [ 6464/60000]\n",
      "loss: 0.679508  [12864/60000]\n",
      "loss: 0.873876  [19264/60000]\n",
      "loss: 0.754668  [25664/60000]\n",
      "loss: 0.765822  [32064/60000]\n",
      "loss: 0.840800  [38464/60000]\n",
      "loss: 0.817748  [44864/60000]\n",
      "loss: 0.809619  [51264/60000]\n",
      "loss: 0.795335  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.789469 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.794672  [   64/60000]\n",
      "loss: 0.873295  [ 6464/60000]\n",
      "loss: 0.640139  [12864/60000]\n",
      "loss: 0.841731  [19264/60000]\n",
      "loss: 0.727789  [25664/60000]\n",
      "loss: 0.732087  [32064/60000]\n",
      "loss: 0.812027  [38464/60000]\n",
      "loss: 0.798777  [44864/60000]\n",
      "loss: 0.781155  [51264/60000]\n",
      "loss: 0.768865  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.762387 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.754990  [   64/60000]\n",
      "loss: 0.841576  [ 6464/60000]\n",
      "loss: 0.607021  [12864/60000]\n",
      "loss: 0.815071  [19264/60000]\n",
      "loss: 0.705356  [25664/60000]\n",
      "loss: 0.704874  [32064/60000]\n",
      "loss: 0.786325  [38464/60000]\n",
      "loss: 0.782833  [44864/60000]\n",
      "loss: 0.757557  [51264/60000]\n",
      "loss: 0.745560  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.739044 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.720333  [   64/60000]\n",
      "loss: 0.812776  [ 6464/60000]\n",
      "loss: 0.578556  [12864/60000]\n",
      "loss: 0.792697  [19264/60000]\n",
      "loss: 0.686077  [25664/60000]\n",
      "loss: 0.682449  [32064/60000]\n",
      "loss: 0.762744  [38464/60000]\n",
      "loss: 0.768819  [44864/60000]\n",
      "loss: 0.737410  [51264/60000]\n",
      "loss: 0.724640  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.718382 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.689605  [   64/60000]\n",
      "loss: 0.786416  [ 6464/60000]\n",
      "loss: 0.553544  [12864/60000]\n",
      "loss: 0.773505  [19264/60000]\n",
      "loss: 0.669366  [25664/60000]\n",
      "loss: 0.663644  [32064/60000]\n",
      "loss: 0.740913  [38464/60000]\n",
      "loss: 0.756060  [44864/60000]\n",
      "loss: 0.719921  [51264/60000]\n",
      "loss: 0.705675  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.699791 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.662122  [   64/60000]\n",
      "loss: 0.762303  [ 6464/60000]\n",
      "loss: 0.531466  [12864/60000]\n",
      "loss: 0.756714  [19264/60000]\n",
      "loss: 0.654737  [25664/60000]\n",
      "loss: 0.647562  [32064/60000]\n",
      "loss: 0.720717  [38464/60000]\n",
      "loss: 0.744257  [44864/60000]\n",
      "loss: 0.704615  [51264/60000]\n",
      "loss: 0.688418  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.682847 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.637628  [   64/60000]\n",
      "loss: 0.740222  [ 6464/60000]\n",
      "loss: 0.511763  [12864/60000]\n",
      "loss: 0.741781  [19264/60000]\n",
      "loss: 0.641900  [25664/60000]\n",
      "loss: 0.633868  [32064/60000]\n",
      "loss: 0.701846  [38464/60000]\n",
      "loss: 0.733293  [44864/60000]\n",
      "loss: 0.691050  [51264/60000]\n",
      "loss: 0.672499  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.667320 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.615650  [   64/60000]\n",
      "loss: 0.720011  [ 6464/60000]\n",
      "loss: 0.494177  [12864/60000]\n",
      "loss: 0.728219  [19264/60000]\n",
      "loss: 0.630611  [25664/60000]\n",
      "loss: 0.621919  [32064/60000]\n",
      "loss: 0.684208  [38464/60000]\n",
      "loss: 0.723218  [44864/60000]\n",
      "loss: 0.679123  [51264/60000]\n",
      "loss: 0.657784  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.653077 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.595945  [   64/60000]\n",
      "loss: 0.701508  [ 6464/60000]\n",
      "loss: 0.478311  [12864/60000]\n",
      "loss: 0.715767  [19264/60000]\n",
      "loss: 0.620568  [25664/60000]\n",
      "loss: 0.611362  [32064/60000]\n",
      "loss: 0.667838  [38464/60000]\n",
      "loss: 0.714110  [44864/60000]\n",
      "loss: 0.668839  [51264/60000]\n",
      "loss: 0.644258  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.640011 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.578178  [   64/60000]\n",
      "loss: 0.684530  [ 6464/60000]\n",
      "loss: 0.464059  [12864/60000]\n",
      "loss: 0.704295  [19264/60000]\n",
      "loss: 0.611681  [25664/60000]\n",
      "loss: 0.602017  [32064/60000]\n",
      "loss: 0.652683  [38464/60000]\n",
      "loss: 0.706094  [44864/60000]\n",
      "loss: 0.659969  [51264/60000]\n",
      "loss: 0.631797  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.628017 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.562187  [   64/60000]\n",
      "loss: 0.668978  [ 6464/60000]\n",
      "loss: 0.451290  [12864/60000]\n",
      "loss: 0.693682  [19264/60000]\n",
      "loss: 0.603606  [25664/60000]\n",
      "loss: 0.593745  [32064/60000]\n",
      "loss: 0.638634  [38464/60000]\n",
      "loss: 0.699147  [44864/60000]\n",
      "loss: 0.652223  [51264/60000]\n",
      "loss: 0.620342  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.617030 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.547845  [   64/60000]\n",
      "loss: 0.654855  [ 6464/60000]\n",
      "loss: 0.439745  [12864/60000]\n",
      "loss: 0.683722  [19264/60000]\n",
      "loss: 0.596305  [25664/60000]\n",
      "loss: 0.586330  [32064/60000]\n",
      "loss: 0.625663  [38464/60000]\n",
      "loss: 0.693195  [44864/60000]\n",
      "loss: 0.645490  [51264/60000]\n",
      "loss: 0.609702  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.9%, Avg loss: 0.606950 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.534872  [   64/60000]\n",
      "loss: 0.642035  [ 6464/60000]\n",
      "loss: 0.429261  [12864/60000]\n",
      "loss: 0.674301  [19264/60000]\n",
      "loss: 0.589548  [25664/60000]\n",
      "loss: 0.579619  [32064/60000]\n",
      "loss: 0.613683  [38464/60000]\n",
      "loss: 0.688233  [44864/60000]\n",
      "loss: 0.639655  [51264/60000]\n",
      "loss: 0.599680  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.3%, Avg loss: 0.597699 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.523012  [   64/60000]\n",
      "loss: 0.630353  [ 6464/60000]\n",
      "loss: 0.419648  [12864/60000]\n",
      "loss: 0.665443  [19264/60000]\n",
      "loss: 0.583136  [25664/60000]\n",
      "loss: 0.573438  [32064/60000]\n",
      "loss: 0.602664  [38464/60000]\n",
      "loss: 0.684184  [44864/60000]\n",
      "loss: 0.634663  [51264/60000]\n",
      "loss: 0.590304  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.589212 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.512183  [   64/60000]\n",
      "loss: 0.619720  [ 6464/60000]\n",
      "loss: 0.410788  [12864/60000]\n",
      "loss: 0.657049  [19264/60000]\n",
      "loss: 0.576928  [25664/60000]\n",
      "loss: 0.567656  [32064/60000]\n",
      "loss: 0.592570  [38464/60000]\n",
      "loss: 0.680961  [44864/60000]\n",
      "loss: 0.630392  [51264/60000]\n",
      "loss: 0.581555  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.581413 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.502213  [   64/60000]\n",
      "loss: 0.609968  [ 6464/60000]\n",
      "loss: 0.402596  [12864/60000]\n",
      "loss: 0.649074  [19264/60000]\n",
      "loss: 0.570898  [25664/60000]\n",
      "loss: 0.562244  [32064/60000]\n",
      "loss: 0.583336  [38464/60000]\n",
      "loss: 0.678434  [44864/60000]\n",
      "loss: 0.626760  [51264/60000]\n",
      "loss: 0.573196  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.574227 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.492943  [   64/60000]\n",
      "loss: 0.601025  [ 6464/60000]\n",
      "loss: 0.395012  [12864/60000]\n",
      "loss: 0.641501  [19264/60000]\n",
      "loss: 0.565016  [25664/60000]\n",
      "loss: 0.557074  [32064/60000]\n",
      "loss: 0.574833  [38464/60000]\n",
      "loss: 0.676574  [44864/60000]\n",
      "loss: 0.623663  [51264/60000]\n",
      "loss: 0.565209  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.567592 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.484361  [   64/60000]\n",
      "loss: 0.592818  [ 6464/60000]\n",
      "loss: 0.387970  [12864/60000]\n",
      "loss: 0.634305  [19264/60000]\n",
      "loss: 0.559244  [25664/60000]\n",
      "loss: 0.552208  [32064/60000]\n",
      "loss: 0.566959  [38464/60000]\n",
      "loss: 0.675255  [44864/60000]\n",
      "loss: 0.621009  [51264/60000]\n",
      "loss: 0.557477  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.561451 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.476312  [   64/60000]\n",
      "loss: 0.585311  [ 6464/60000]\n",
      "loss: 0.381441  [12864/60000]\n",
      "loss: 0.627480  [19264/60000]\n",
      "loss: 0.553591  [25664/60000]\n",
      "loss: 0.547472  [32064/60000]\n",
      "loss: 0.559694  [38464/60000]\n",
      "loss: 0.674396  [44864/60000]\n",
      "loss: 0.618735  [51264/60000]\n",
      "loss: 0.550025  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.555752 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.468754  [   64/60000]\n",
      "loss: 0.578435  [ 6464/60000]\n",
      "loss: 0.375342  [12864/60000]\n",
      "loss: 0.621014  [19264/60000]\n",
      "loss: 0.548058  [25664/60000]\n",
      "loss: 0.542877  [32064/60000]\n",
      "loss: 0.552967  [38464/60000]\n",
      "loss: 0.673962  [44864/60000]\n",
      "loss: 0.616742  [51264/60000]\n",
      "loss: 0.542849  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.550462 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.461613  [   64/60000]\n",
      "loss: 0.572123  [ 6464/60000]\n",
      "loss: 0.369661  [12864/60000]\n",
      "loss: 0.614850  [19264/60000]\n",
      "loss: 0.542599  [25664/60000]\n",
      "loss: 0.538399  [32064/60000]\n",
      "loss: 0.546702  [38464/60000]\n",
      "loss: 0.673835  [44864/60000]\n",
      "loss: 0.614898  [51264/60000]\n",
      "loss: 0.535965  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.545531 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Run using GPU\n",
    "# Increase the number of epochs to at least 30.\n",
    "epochs = 30\n",
    "total_time_gpu = 0\n",
    "\n",
    "for t in range(epochs):\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "    end_time = time.time()  # End timer\n",
    "    epoch_time = end_time - start_time  # Time taken for this epoch\n",
    "    total_time_gpu += epoch_time\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0186c4-ae23-4b67-89f8-7b358b435629",
   "metadata": {},
   "source": [
    "Confirm with “nvidia-smi -l” that you were indeed using GPU when the program was running.\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  Tesla T4                       On  |   00000000:00:04.0 Off |                    0 |\n",
    "| N/A   62C    P0             29W /   70W |     161MiB /  15360MiB |      5%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "                                                                                         \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "+-----------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ab56eff-fc89-441d-93d6-ac279074d09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.306399  [   64/60000]\n",
      "loss: 2.294306  [ 6464/60000]\n",
      "loss: 2.285729  [12864/60000]\n",
      "loss: 2.279481  [19264/60000]\n",
      "loss: 2.271997  [25664/60000]\n",
      "loss: 2.234895  [32064/60000]\n",
      "loss: 2.237722  [38464/60000]\n",
      "loss: 2.212082  [44864/60000]\n",
      "loss: 2.204960  [51264/60000]\n",
      "loss: 2.180465  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.6%, Avg loss: 2.174642 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.180748  [   64/60000]\n",
      "loss: 2.166188  [ 6464/60000]\n",
      "loss: 2.122184  [12864/60000]\n",
      "loss: 2.140046  [19264/60000]\n",
      "loss: 2.095920  [25664/60000]\n",
      "loss: 2.032320  [32064/60000]\n",
      "loss: 2.056299  [38464/60000]\n",
      "loss: 1.987244  [44864/60000]\n",
      "loss: 1.986756  [51264/60000]\n",
      "loss: 1.925987  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 1.920554 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.949208  [   64/60000]\n",
      "loss: 1.906153  [ 6464/60000]\n",
      "loss: 1.809785  [12864/60000]\n",
      "loss: 1.852741  [19264/60000]\n",
      "loss: 1.747474  [25664/60000]\n",
      "loss: 1.691912  [32064/60000]\n",
      "loss: 1.710583  [38464/60000]\n",
      "loss: 1.619795  [44864/60000]\n",
      "loss: 1.639885  [51264/60000]\n",
      "loss: 1.536060  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 1.555612 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.622251  [   64/60000]\n",
      "loss: 1.568102  [ 6464/60000]\n",
      "loss: 1.442503  [12864/60000]\n",
      "loss: 1.512110  [19264/60000]\n",
      "loss: 1.397087  [25664/60000]\n",
      "loss: 1.381914  [32064/60000]\n",
      "loss: 1.390702  [38464/60000]\n",
      "loss: 1.325190  [44864/60000]\n",
      "loss: 1.356837  [51264/60000]\n",
      "loss: 1.248636  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.283948 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.368139  [   64/60000]\n",
      "loss: 1.325319  [ 6464/60000]\n",
      "loss: 1.185847  [12864/60000]\n",
      "loss: 1.286196  [19264/60000]\n",
      "loss: 1.163334  [25664/60000]\n",
      "loss: 1.177967  [32064/60000]\n",
      "loss: 1.193390  [38464/60000]\n",
      "loss: 1.139080  [44864/60000]\n",
      "loss: 1.176213  [51264/60000]\n",
      "loss: 1.081742  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.113188 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.195739  [   64/60000]\n",
      "loss: 1.169630  [ 6464/60000]\n",
      "loss: 1.013701  [12864/60000]\n",
      "loss: 1.144580  [19264/60000]\n",
      "loss: 1.013832  [25664/60000]\n",
      "loss: 1.038929  [32064/60000]\n",
      "loss: 1.071698  [38464/60000]\n",
      "loss: 1.017715  [44864/60000]\n",
      "loss: 1.055666  [51264/60000]\n",
      "loss: 0.977216  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 1.001537 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.073845  [   64/60000]\n",
      "loss: 1.067655  [ 6464/60000]\n",
      "loss: 0.893640  [12864/60000]\n",
      "loss: 1.049396  [19264/60000]\n",
      "loss: 0.917719  [25664/60000]\n",
      "loss: 0.940240  [32064/60000]\n",
      "loss: 0.992220  [38464/60000]\n",
      "loss: 0.936386  [44864/60000]\n",
      "loss: 0.970718  [51264/60000]\n",
      "loss: 0.907416  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.925000 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.982583  [   64/60000]\n",
      "loss: 0.996786  [ 6464/60000]\n",
      "loss: 0.806899  [12864/60000]\n",
      "loss: 0.981894  [19264/60000]\n",
      "loss: 0.854192  [25664/60000]\n",
      "loss: 0.868168  [32064/60000]\n",
      "loss: 0.936932  [38464/60000]\n",
      "loss: 0.881025  [44864/60000]\n",
      "loss: 0.909490  [51264/60000]\n",
      "loss: 0.857026  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.870181 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.911581  [   64/60000]\n",
      "loss: 0.943960  [ 6464/60000]\n",
      "loss: 0.741884  [12864/60000]\n",
      "loss: 0.931262  [19264/60000]\n",
      "loss: 0.809684  [25664/60000]\n",
      "loss: 0.814216  [32064/60000]\n",
      "loss: 0.895335  [38464/60000]\n",
      "loss: 0.842117  [44864/60000]\n",
      "loss: 0.863805  [51264/60000]\n",
      "loss: 0.818129  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 0.828971 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.854437  [   64/60000]\n",
      "loss: 0.901793  [ 6464/60000]\n",
      "loss: 0.691329  [12864/60000]\n",
      "loss: 0.891679  [19264/60000]\n",
      "loss: 0.776601  [25664/60000]\n",
      "loss: 0.773262  [32064/60000]\n",
      "loss: 0.861787  [38464/60000]\n",
      "loss: 0.813668  [44864/60000]\n",
      "loss: 0.828733  [51264/60000]\n",
      "loss: 0.786820  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.796520 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.807156  [   64/60000]\n",
      "loss: 0.866338  [ 6464/60000]\n",
      "loss: 0.650681  [12864/60000]\n",
      "loss: 0.859640  [19264/60000]\n",
      "loss: 0.750641  [25664/60000]\n",
      "loss: 0.741423  [32064/60000]\n",
      "loss: 0.833162  [38464/60000]\n",
      "loss: 0.791570  [44864/60000]\n",
      "loss: 0.800838  [51264/60000]\n",
      "loss: 0.760561  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.769788 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.766949  [   64/60000]\n",
      "loss: 0.835333  [ 6464/60000]\n",
      "loss: 0.616850  [12864/60000]\n",
      "loss: 0.833011  [19264/60000]\n",
      "loss: 0.729358  [25664/60000]\n",
      "loss: 0.715981  [32064/60000]\n",
      "loss: 0.807867  [38464/60000]\n",
      "loss: 0.773522  [44864/60000]\n",
      "loss: 0.777854  [51264/60000]\n",
      "loss: 0.737818  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.746908 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.732097  [   64/60000]\n",
      "loss: 0.807254  [ 6464/60000]\n",
      "loss: 0.588120  [12864/60000]\n",
      "loss: 0.810321  [19264/60000]\n",
      "loss: 0.711087  [25664/60000]\n",
      "loss: 0.695210  [32064/60000]\n",
      "loss: 0.784760  [38464/60000]\n",
      "loss: 0.758003  [44864/60000]\n",
      "loss: 0.758351  [51264/60000]\n",
      "loss: 0.717541  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.726708 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.701318  [   64/60000]\n",
      "loss: 0.781568  [ 6464/60000]\n",
      "loss: 0.563168  [12864/60000]\n",
      "loss: 0.790618  [19264/60000]\n",
      "loss: 0.694906  [25664/60000]\n",
      "loss: 0.677777  [32064/60000]\n",
      "loss: 0.763327  [38464/60000]\n",
      "loss: 0.744245  [44864/60000]\n",
      "loss: 0.741413  [51264/60000]\n",
      "loss: 0.699038  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.7%, Avg loss: 0.708467 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.673796  [   64/60000]\n",
      "loss: 0.757959  [ 6464/60000]\n",
      "loss: 0.541010  [12864/60000]\n",
      "loss: 0.773150  [19264/60000]\n",
      "loss: 0.680541  [25664/60000]\n",
      "loss: 0.662880  [32064/60000]\n",
      "loss: 0.743215  [38464/60000]\n",
      "loss: 0.731693  [44864/60000]\n",
      "loss: 0.726335  [51264/60000]\n",
      "loss: 0.682117  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.691747 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.648994  [   64/60000]\n",
      "loss: 0.736202  [ 6464/60000]\n",
      "loss: 0.521268  [12864/60000]\n",
      "loss: 0.757381  [19264/60000]\n",
      "loss: 0.667635  [25664/60000]\n",
      "loss: 0.649782  [32064/60000]\n",
      "loss: 0.724300  [38464/60000]\n",
      "loss: 0.720293  [44864/60000]\n",
      "loss: 0.712978  [51264/60000]\n",
      "loss: 0.666502  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.676306 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.626495  [   64/60000]\n",
      "loss: 0.716052  [ 6464/60000]\n",
      "loss: 0.503609  [12864/60000]\n",
      "loss: 0.742984  [19264/60000]\n",
      "loss: 0.656156  [25664/60000]\n",
      "loss: 0.638155  [32064/60000]\n",
      "loss: 0.706481  [38464/60000]\n",
      "loss: 0.709938  [44864/60000]\n",
      "loss: 0.700978  [51264/60000]\n",
      "loss: 0.651971  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.662012 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.606054  [   64/60000]\n",
      "loss: 0.697363  [ 6464/60000]\n",
      "loss: 0.487681  [12864/60000]\n",
      "loss: 0.729629  [19264/60000]\n",
      "loss: 0.645729  [25664/60000]\n",
      "loss: 0.627792  [32064/60000]\n",
      "loss: 0.689800  [38464/60000]\n",
      "loss: 0.700633  [44864/60000]\n",
      "loss: 0.690384  [51264/60000]\n",
      "loss: 0.638359  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.648757 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.587326  [   64/60000]\n",
      "loss: 0.680075  [ 6464/60000]\n",
      "loss: 0.473305  [12864/60000]\n",
      "loss: 0.717290  [19264/60000]\n",
      "loss: 0.636300  [25664/60000]\n",
      "loss: 0.618486  [32064/60000]\n",
      "loss: 0.674205  [38464/60000]\n",
      "loss: 0.692470  [44864/60000]\n",
      "loss: 0.681176  [51264/60000]\n",
      "loss: 0.625687  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.636477 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.570178  [   64/60000]\n",
      "loss: 0.664132  [ 6464/60000]\n",
      "loss: 0.460237  [12864/60000]\n",
      "loss: 0.705837  [19264/60000]\n",
      "loss: 0.627736  [25664/60000]\n",
      "loss: 0.610180  [32064/60000]\n",
      "loss: 0.659630  [38464/60000]\n",
      "loss: 0.685422  [44864/60000]\n",
      "loss: 0.673197  [51264/60000]\n",
      "loss: 0.613831  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.625121 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.554452  [   64/60000]\n",
      "loss: 0.649473  [ 6464/60000]\n",
      "loss: 0.448317  [12864/60000]\n",
      "loss: 0.695157  [19264/60000]\n",
      "loss: 0.619978  [25664/60000]\n",
      "loss: 0.602644  [32064/60000]\n",
      "loss: 0.646111  [38464/60000]\n",
      "loss: 0.679410  [44864/60000]\n",
      "loss: 0.666241  [51264/60000]\n",
      "loss: 0.602715  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.614623 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.540059  [   64/60000]\n",
      "loss: 0.635953  [ 6464/60000]\n",
      "loss: 0.437409  [12864/60000]\n",
      "loss: 0.685161  [19264/60000]\n",
      "loss: 0.612895  [25664/60000]\n",
      "loss: 0.595707  [32064/60000]\n",
      "loss: 0.633607  [38464/60000]\n",
      "loss: 0.674429  [44864/60000]\n",
      "loss: 0.660298  [51264/60000]\n",
      "loss: 0.592237  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.604936 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.526768  [   64/60000]\n",
      "loss: 0.623433  [ 6464/60000]\n",
      "loss: 0.427470  [12864/60000]\n",
      "loss: 0.675823  [19264/60000]\n",
      "loss: 0.606321  [25664/60000]\n",
      "loss: 0.589307  [32064/60000]\n",
      "loss: 0.622007  [38464/60000]\n",
      "loss: 0.670378  [44864/60000]\n",
      "loss: 0.655247  [51264/60000]\n",
      "loss: 0.582261  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.595989 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.514483  [   64/60000]\n",
      "loss: 0.611824  [ 6464/60000]\n",
      "loss: 0.418381  [12864/60000]\n",
      "loss: 0.667057  [19264/60000]\n",
      "loss: 0.600089  [25664/60000]\n",
      "loss: 0.583332  [32064/60000]\n",
      "loss: 0.611281  [38464/60000]\n",
      "loss: 0.667254  [44864/60000]\n",
      "loss: 0.650896  [51264/60000]\n",
      "loss: 0.572726  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.587727 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.503032  [   64/60000]\n",
      "loss: 0.601159  [ 6464/60000]\n",
      "loss: 0.409962  [12864/60000]\n",
      "loss: 0.658758  [19264/60000]\n",
      "loss: 0.593997  [25664/60000]\n",
      "loss: 0.577740  [32064/60000]\n",
      "loss: 0.601376  [38464/60000]\n",
      "loss: 0.664945  [44864/60000]\n",
      "loss: 0.647218  [51264/60000]\n",
      "loss: 0.563679  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.580093 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.492299  [   64/60000]\n",
      "loss: 0.591323  [ 6464/60000]\n",
      "loss: 0.402151  [12864/60000]\n",
      "loss: 0.650938  [19264/60000]\n",
      "loss: 0.588096  [25664/60000]\n",
      "loss: 0.572413  [32064/60000]\n",
      "loss: 0.592227  [38464/60000]\n",
      "loss: 0.663297  [44864/60000]\n",
      "loss: 0.644104  [51264/60000]\n",
      "loss: 0.555006  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.573033 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.482284  [   64/60000]\n",
      "loss: 0.582219  [ 6464/60000]\n",
      "loss: 0.394909  [12864/60000]\n",
      "loss: 0.643500  [19264/60000]\n",
      "loss: 0.582276  [25664/60000]\n",
      "loss: 0.567307  [32064/60000]\n",
      "loss: 0.583765  [38464/60000]\n",
      "loss: 0.662233  [44864/60000]\n",
      "loss: 0.641433  [51264/60000]\n",
      "loss: 0.546555  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.566487 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.472936  [   64/60000]\n",
      "loss: 0.573779  [ 6464/60000]\n",
      "loss: 0.388161  [12864/60000]\n",
      "loss: 0.636422  [19264/60000]\n",
      "loss: 0.576607  [25664/60000]\n",
      "loss: 0.562341  [32064/60000]\n",
      "loss: 0.575922  [38464/60000]\n",
      "loss: 0.661600  [44864/60000]\n",
      "loss: 0.639024  [51264/60000]\n",
      "loss: 0.538392  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.560408 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.464220  [   64/60000]\n",
      "loss: 0.565940  [ 6464/60000]\n",
      "loss: 0.381789  [12864/60000]\n",
      "loss: 0.629640  [19264/60000]\n",
      "loss: 0.570983  [25664/60000]\n",
      "loss: 0.557520  [32064/60000]\n",
      "loss: 0.568672  [38464/60000]\n",
      "loss: 0.661433  [44864/60000]\n",
      "loss: 0.636952  [51264/60000]\n",
      "loss: 0.530422  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.554757 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.455970  [   64/60000]\n",
      "loss: 0.558682  [ 6464/60000]\n",
      "loss: 0.375958  [12864/60000]\n",
      "loss: 0.623175  [19264/60000]\n",
      "loss: 0.565430  [25664/60000]\n",
      "loss: 0.552845  [32064/60000]\n",
      "loss: 0.561964  [38464/60000]\n",
      "loss: 0.661651  [44864/60000]\n",
      "loss: 0.635108  [51264/60000]\n",
      "loss: 0.522709  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.549504 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Run using CPU\n",
    "device = 'cpu'\n",
    "\n",
    "# Move the model to the CPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_time_cpu = 0\n",
    "\n",
    "for t in range(epochs):\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "    end_time = time.time()  # End timer\n",
    "    epoch_time = end_time - start_time  # Time taken for this epoch\n",
    "    total_time_cpu += epoch_time\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84a1cece-ca05-4bbc-80cf-f2041c65508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# How much faster does it run on GPU?\n",
    "print(round(total_time_cpu-total_time_gpu,2),'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9143e845-3d8c-4339-987a-444632108119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 80.9% after 30 epochs\n"
     ]
    }
   ],
   "source": [
    "# What accuracy is achieved after 30 epochs?\n",
    "print('The accuracy is 80.9% after 30 epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39bf95-c721-4990-b58b-99a92deffd55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e18561c7-d8d8-41c8-a213-593c46f0d69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Size: 100x100, Memory Usage: 8641024 / 15642329088 (0.0552%)\n",
      "Matrix Size: 1100x1100, Memory Usage: 23041024 / 15642329088 (0.1473%)\n",
      "Matrix Size: 2100x2100, Memory Usage: 61441024 / 15642329088 (0.3928%)\n",
      "Matrix Size: 3100x3100, Memory Usage: 123841024 / 15642329088 (0.7917%)\n",
      "Matrix Size: 4100x4100, Memory Usage: 210241024 / 15642329088 (1.3441%)\n",
      "Matrix Size: 5100x5100, Memory Usage: 323092480 / 15642329088 (2.0655%)\n",
      "Matrix Size: 6100x6100, Memory Usage: 455213056 / 15642329088 (2.9101%)\n",
      "Matrix Size: 7100x7100, Memory Usage: 613441024 / 15642329088 (3.9217%)\n",
      "Matrix Size: 8100x8100, Memory Usage: 795841024 / 15642329088 (5.0877%)\n",
      "Matrix Size: 9100x9100, Memory Usage: 1002569728 / 15642329088 (6.4093%)\n",
      "Matrix Size: 10100x10100, Memory Usage: 1235353600 / 15642329088 (7.8975%)\n",
      "Matrix Size: 11100x11100, Memory Usage: 1487041024 / 15642329088 (9.5065%)\n",
      "Matrix Size: 12100x12100, Memory Usage: 1765441024 / 15642329088 (11.2863%)\n",
      "Matrix Size: 13100x13100, Memory Usage: 2067841024 / 15642329088 (13.2195%)\n",
      "Matrix Size: 14100x14100, Memory Usage: 2394241024 / 15642329088 (15.3062%)\n",
      "Matrix Size: 15100x15100, Memory Usage: 2745303040 / 15642329088 (17.5505%)\n",
      "Matrix Size: 16100x16100, Memory Usage: 3119041024 / 15642329088 (19.9397%)\n",
      "Matrix Size: 17100x17100, Memory Usage: 3519152128 / 15642329088 (22.4976%)\n",
      "Matrix Size: 18100x18100, Memory Usage: 3940679680 / 15642329088 (25.1924%)\n",
      "Matrix Size: 19100x19100, Memory Usage: 4387373056 / 15642329088 (28.0481%)\n",
      "Matrix Size: 20100x20100, Memory Usage: 4859232256 / 15642329088 (31.0646%)\n",
      "Matrix Size: 21100x21100, Memory Usage: 5351041024 / 15642329088 (34.2087%)\n",
      "Matrix Size: 22100x22100, Memory Usage: 5872156672 / 15642329088 (37.5402%)\n",
      "Matrix Size: 23100x23100, Memory Usage: 6413221888 / 15642329088 (40.9991%)\n",
      "Matrix Size: 24100x24100, Memory Usage: 6979452928 / 15642329088 (44.6190%)\n",
      "Matrix Size: 25100x25100, Memory Usage: 7570849792 / 15642329088 (48.3998%)\n",
      "Matrix Size: 26100x26100, Memory Usage: 8183041024 / 15642329088 (52.3134%)\n",
      "Matrix Size: 27100x27100, Memory Usage: 8822849536 / 15642329088 (56.4037%)\n",
      "Matrix Size: 28100x28100, Memory Usage: 9483841024 / 15642329088 (60.6293%)\n",
      "Matrix Size: 29100x29100, Memory Usage: 10170241024 / 15642329088 (65.0174%)\n",
      "Matrix Size: 30100x30100, Memory Usage: 10880641024 / 15642329088 (69.5590%)\n",
      "Matrix Size: 31100x31100, Memory Usage: 11616256000 / 15642329088 (74.2617%)\n",
      "Matrix Size: 32100x32100, Memory Usage: 12373441024 / 15642329088 (79.1023%)\n",
      "Matrix Size: 33100x33100, Memory Usage: 13157662720 / 15642329088 (84.1158%)\n",
      "Matrix Size: 34100x34100, Memory Usage: 13962969088 / 15642329088 (89.2640%)\n",
      "Matrix Size: 35100x35100, Memory Usage: 14793441280 / 15642329088 (94.5731%)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.86 GiB (GPU 0; 14.57 GiB total capacity; 9.72 GiB already allocated; 4.73 GiB free; 9.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m matrix_B \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(matrix_size, matrix_size, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Perform matrix multiplication\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m matrix_product \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix_B\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Get the currently allocated memory on the GPU\u001b[39;00m\n\u001b[1;32m     20\u001b[0m current_memory_usage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(device)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.86 GiB (GPU 0; 14.57 GiB total capacity; 9.72 GiB already allocated; 4.73 GiB free; 9.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Get the total memory available on the GPU\n",
    "gpu_total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "\n",
    "# Loop to increase matrix size until GPU memory runs out\n",
    "for iteration in range(0,500):\n",
    "    matrix_size = 100 + iteration * 1000  # Increase matrix size progressively\n",
    "    \n",
    "    # Generate two random matrices of the current size\n",
    "    matrix_A = torch.randn(matrix_size, matrix_size, device=device)\n",
    "    matrix_B = torch.randn(matrix_size, matrix_size, device=device)\n",
    "    \n",
    "    # Perform matrix multiplication\n",
    "    matrix_product = torch.matmul(matrix_A, matrix_B)\n",
    "    \n",
    "    # Get the currently allocated memory on the GPU\n",
    "    current_memory_usage = torch.cuda.memory_allocated(device)\n",
    "    \n",
    "    # Calculate the percentage of memory used\n",
    "    memory_usage_percentage = current_memory_usage / gpu_total_memory * 100\n",
    "    \n",
    "    # Print the current matrix size and memory usage\n",
    "    print(f\"Matrix Size: {matrix_size}x{matrix_size}, \"\n",
    "          f\"Memory Usage: {current_memory_usage} / {gpu_total_memory} \"\n",
    "          f\"({memory_usage_percentage:.4f}%)\")\n",
    "    \n",
    "    # Check if the GPU memory usage exceeds 99%\n",
    "    if memory_usage_percentage > 99:\n",
    "        print(f\"Memory limit reached at matrix size: {matrix_size}\")\n",
    "        break\n",
    "    \n",
    "    # Clean up by deleting the matrix product and freeing memory\n",
    "    del matrix_product\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93cee71-e653-494e-8f85-2935b6d54e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix size: 1000x1000\n",
      "Average GPU duration: 0.01 seconds\n",
      "Average CPU duration: 0.01 seconds\n",
      "----------------------------\n",
      "Matrix size: 5000x5000\n",
      "Average GPU duration: 0.00 seconds\n",
      "Average CPU duration: 0.40 seconds\n",
      "----------------------------\n",
      "Matrix size: 10000x10000\n",
      "Average GPU duration: 0.00 seconds\n",
      "Average CPU duration: 3.15 seconds\n",
      "----------------------------\n",
      "Matrix size: 20000x20000\n",
      "Average GPU duration: 0.00 seconds\n",
      "Average CPU duration: 25.34 seconds\n",
      "----------------------------\n",
      "Matrix size: 35100x35100\n",
      "Average GPU duration: 0.08 seconds\n",
      "Average CPU duration: 136.83 seconds\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# List of 5 matrix sizes (including the maximum size that GPU can handle)\n",
    "matrix_sizes = [1000, 5000, 10000, 20000, 35100]  # Modify this based on your GPU limits\n",
    "\n",
    "# Function to measure time for matrix multiplication on GPU and CPU\n",
    "def measure_time(matrix_size):\n",
    "    # Generate random matrices on the GPU\n",
    "    matrix_A = torch.randn(matrix_size, matrix_size, device=device)\n",
    "    matrix_B = torch.randn(matrix_size, matrix_size, device=device)\n",
    "\n",
    "    # GPU multiplication\n",
    "    start_gpu = time.time()\n",
    "    result_gpu = torch.matmul(matrix_A, matrix_B)\n",
    "    end_gpu = time.time()\n",
    "    gpu_time = end_gpu - start_gpu\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Move matrices to CPU for CPU multiplication\n",
    "    A_cpu = matrix_A.to('cpu')\n",
    "    B_cpu = matrix_B.to('cpu')\n",
    "\n",
    "    # CPU multiplication\n",
    "    start_cpu = time.time()\n",
    "    result_cpu = torch.matmul(A_cpu, B_cpu)\n",
    "    end_cpu = time.time()\n",
    "    cpu_time = end_cpu - start_cpu\n",
    "\n",
    "    return gpu_time, cpu_time\n",
    "\n",
    "# Run the experiment for each matrix size and repeat 3 times\n",
    "for matrix_size in matrix_sizes:\n",
    "    gpu_times = []\n",
    "    cpu_times = []\n",
    "    for _ in range(3):\n",
    "        gpu_time, cpu_time = measure_time(matrix_size)\n",
    "        gpu_times.append(gpu_time)\n",
    "        cpu_times.append(cpu_time)\n",
    "    \n",
    "    # Calculate and print the average time for GPU and CPU\n",
    "    print(f\"Matrix size: {matrix_size}x{matrix_size}\")\n",
    "    print(f\"Average GPU duration: {sum(gpu_times) / 3:.2f} seconds\")\n",
    "    print(f\"Average CPU duration: {sum(cpu_times) / 3:.2f} seconds\")\n",
    "    print('----------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c842be-e907-464d-a7ff-57b9197e0650",
   "metadata": {},
   "source": [
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  Tesla T4                       On  |   00000000:00:04.0 Off |                    0 |\n",
    "| N/A   82C    P0             70W /   70W |   14235MiB /  15360MiB |    100%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "                                                                                         \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "+-----------------------------------------------------------------------------------------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4522a0c6-630e-45bd-8b6f-764c849aa072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU load is 100%.\n",
      "Memory Usage: 14,235 MiB / 15,360 MiB\n"
     ]
    }
   ],
   "source": [
    "# What was the load on GPU? How much GPU memory did you program use?\n",
    "print('GPU load is 100%.')\n",
    "print('Memory Usage: 14,235 MiB / 15,360 MiB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488fef3e-5a7c-4d4a-bbe5-34d15c266457",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02673b57-a0fd-4a0c-b88d-1a4aa8b6a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://download.pytorch.org/tutorial/hymenoptera_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9193366d-ca64-4a61-87ee-18d4cd42c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data; mkdir -p data; cd data; unzip ../hymenoptera_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaa90fce-1330-4534-84d4-bfce5d2c1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'data/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e058d9d-5335-47d0-bf29-87859a996956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.8942 Acc: 0.5000\n",
      "val Loss: 0.6939 Acc: 0.5621\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.7341 Acc: 0.5820\n",
      "val Loss: 0.7231 Acc: 0.5686\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.7771 Acc: 0.5779\n",
      "val Loss: 0.6783 Acc: 0.5425\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.6935 Acc: 0.6066\n",
      "val Loss: 0.6131 Acc: 0.6797\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.6814 Acc: 0.5902\n",
      "val Loss: 0.6164 Acc: 0.7255\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.7380 Acc: 0.5492\n",
      "val Loss: 0.6299 Acc: 0.6667\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.6694 Acc: 0.5902\n",
      "val Loss: 0.6025 Acc: 0.7059\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.6183 Acc: 0.6189\n",
      "val Loss: 0.5987 Acc: 0.7451\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.6026 Acc: 0.6721\n",
      "val Loss: 0.5920 Acc: 0.7320\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.6033 Acc: 0.6680\n",
      "val Loss: 0.5895 Acc: 0.7190\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.6086 Acc: 0.6680\n",
      "val Loss: 0.5705 Acc: 0.7320\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.5994 Acc: 0.7049\n",
      "val Loss: 0.5872 Acc: 0.7255\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.5585 Acc: 0.7254\n",
      "val Loss: 0.5564 Acc: 0.7190\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.5722 Acc: 0.6680\n",
      "val Loss: 0.5504 Acc: 0.7320\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.5432 Acc: 0.7336\n",
      "val Loss: 0.5491 Acc: 0.7451\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.6072 Acc: 0.6311\n",
      "val Loss: 0.5401 Acc: 0.7582\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.5788 Acc: 0.7049\n",
      "val Loss: 0.5511 Acc: 0.7386\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.5531 Acc: 0.7049\n",
      "val Loss: 0.5683 Acc: 0.7190\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.5381 Acc: 0.7295\n",
      "val Loss: 0.5638 Acc: 0.7124\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.5879 Acc: 0.6762\n",
      "val Loss: 0.5504 Acc: 0.7320\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.5801 Acc: 0.6926\n",
      "val Loss: 0.5428 Acc: 0.7516\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.5452 Acc: 0.7049\n",
      "val Loss: 0.5505 Acc: 0.7386\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.5558 Acc: 0.7254\n",
      "val Loss: 0.5518 Acc: 0.7320\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.5918 Acc: 0.7254\n",
      "val Loss: 0.5525 Acc: 0.7451\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.5500 Acc: 0.7172\n",
      "val Loss: 0.5692 Acc: 0.7190\n",
      "\n",
      "Training complete in 0m 55s\n",
      "Best val Acc: 0.758170\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "# Define the custom CNN architecture\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 512)  # Assuming input image size is 224x224\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.pool(self.bn1(torch.relu(self.conv1(x))))\n",
    "        x = self.pool(self.bn2(torch.relu(self.conv2(x))))\n",
    "        x = self.pool(self.bn3(torch.relu(self.conv3(x))))\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.view(-1, 128 * 28 * 28)\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Data augmentation and normalization\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'data/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model, define criterion, optimizer, and scheduler\n",
    "model = CustomCNN(num_classes=len(class_names))\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "    \n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 25\n",
    "trained_model = train_model(model, criterion, optimizer, scheduler, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a5fe33d-d60b-4cec-99c3-0e37859b32c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.8%\n"
     ]
    }
   ],
   "source": [
    "# What accuracy did you manage to achieve?\n",
    "print('75.8%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e15def-f7d3-4789-813d-b94b20b827e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5174bc96-6ea3-4cec-b26c-0c188a38bdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, LSTM, Dense, Attention, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Generate date-based data\n",
    "date_range = pd.date_range(start='1950-01-01', end='2050-12-31', freq='D')\n",
    "\n",
    "# Input data: list of date strings\n",
    "input_dates = date_range.strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "# Output data: formatted date strings with '<eos>' token\n",
    "target_dates = [date.strftime('%d-%m-%Y') + '<eos>' for date in date_range]\n",
    "\n",
    "# Shuffle the data\n",
    "shuffled_data = list(zip(input_dates, target_dates))\n",
    "random.shuffle(shuffled_data)\n",
    "input_dates, target_dates = zip(*shuffled_data)\n",
    "input_dates = list(input_dates)\n",
    "target_dates = list(target_dates)\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "total_samples = len(input_dates)\n",
    "test_size = int(0.1 * total_samples)\n",
    "val_size = int(0.1 * total_samples)\n",
    "train_size = total_samples - test_size - val_size\n",
    "\n",
    "train_inputs = input_dates[:train_size]\n",
    "train_targets = target_dates[:train_size]\n",
    "val_inputs = input_dates[train_size:train_size + val_size]\n",
    "val_targets = target_dates[train_size:train_size + val_size]\n",
    "test_inputs = input_dates[train_size + val_size:]\n",
    "test_targets = target_dates[train_size + val_size:]\n",
    "\n",
    "special_tokens = ['<pad>', '<eos>'] \n",
    "\n",
    "# Tokenize characters, add special tokens, and create mappings\n",
    "unique_characters = set(''.join(train_inputs + train_targets + val_inputs + val_targets + test_inputs + test_targets))\n",
    "unique_tokens = sorted(unique_characters.union(set(special_tokens)))\n",
    "unique_tokens = sorted(set(unique_tokens) - {'<pad>'})\n",
    "\n",
    "token2idx = {'<pad>': 0}\n",
    "for idx, token in enumerate(unique_tokens):\n",
    "    token2idx[token] = idx + 1  # Start from 1\n",
    "idx2token = {idx: token for token, idx in token2idx.items()}\n",
    "\n",
    "num_tokens = len(token2idx)\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# Function to encode date strings into token indices\n",
    "def encode_dates(dates, token2idx, special_tokens):\n",
    "    encoded_sequences = []\n",
    "\n",
    "    for date in dates:\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(date):\n",
    "            matched = False\n",
    "            for token in special_tokens:\n",
    "                if date[i:i+len(token)] == token:\n",
    "                    tokens.append(token)\n",
    "                    i += len(token)\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                tokens.append(date[i])\n",
    "                i += 1\n",
    "        encoded_sequences.append([token2idx[token] for token in tokens])\n",
    "\n",
    "    return encoded_sequences\n",
    "\n",
    "# Encode and pad the sequences\n",
    "train_inputs_encoded = encode_dates(train_inputs, token2idx, special_tokens)\n",
    "train_targets_encoded = encode_dates(train_targets, token2idx, special_tokens)\n",
    "\n",
    "max_input_length = max([len(seq) for seq in train_inputs_encoded])\n",
    "max_target_length = max([len(seq) for seq in train_targets_encoded])\n",
    "\n",
    "train_inputs_padded = pad_sequences(\n",
    "    train_inputs_encoded, maxlen=max_input_length, padding='post', value=token2idx['<pad>']\n",
    ")\n",
    "train_targets_padded = pad_sequences(\n",
    "    train_targets_encoded, maxlen=max_target_length, padding='post', value=token2idx['<pad>']\n",
    ")\n",
    "\n",
    "train_decoder_inputs = np.full_like(train_targets_padded, fill_value=token2idx['<pad>'])\n",
    "train_decoder_inputs[:, 1:] = train_targets_padded[:, :-1]\n",
    "\n",
    "train_decoder_targets = np.expand_dims(train_targets_padded, -1)\n",
    "\n",
    "# Validation encoding and padding\n",
    "val_inputs_encoded = encode_dates(val_inputs, token2idx, special_tokens)\n",
    "val_targets_encoded = encode_dates(val_targets, token2idx, special_tokens)\n",
    "\n",
    "val_inputs_padded = pad_sequences(\n",
    "    val_inputs_encoded, maxlen=max_input_length, padding='post', value=token2idx['<pad>']\n",
    ")\n",
    "val_targets_padded = pad_sequences(\n",
    "    val_targets_encoded, maxlen=max_target_length, padding='post', value=token2idx['<pad>']\n",
    ")\n",
    "\n",
    "val_decoder_inputs = np.full_like(val_targets_padded, fill_value=token2idx['<pad>'])\n",
    "val_decoder_inputs[:, 1:] = val_targets_padded[:, :-1]\n",
    "val_decoder_targets = np.expand_dims(val_targets_padded, -1)\n",
    "\n",
    "# Test encoding and padding\n",
    "test_inputs_encoded = encode_dates(test_inputs, token2idx, special_tokens)\n",
    "test_targets_encoded = encode_dates(test_targets, token2idx, special_tokens)\n",
    "\n",
    "test_inputs_padded = pad_sequences(\n",
    "    test_inputs_encoded, maxlen=max_input_length, padding='post', value=token2idx['<pad>']\n",
    ")\n",
    "test_targets_padded = pad_sequences(\n",
    "    test_targets_encoded, maxlen=max_target_length, padding='post', value=token2idx['<pad>']\n",
    ")\n",
    "\n",
    "# Dataset class to handle batching\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class DateTranslationDataset(Dataset):\n",
    "    def __init__(self, encoder_inputs, decoder_inputs=None, decoder_targets=None):\n",
    "        self.encoder_inputs = encoder_inputs\n",
    "        self.decoder_inputs = decoder_inputs\n",
    "        self.decoder_targets = decoder_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoder_inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'encoder_input': self.encoder_inputs[idx]}\n",
    "        \n",
    "        if self.decoder_inputs is not None and self.decoder_targets is not None:\n",
    "            sample['decoder_input'] = self.decoder_inputs[idx]\n",
    "            sample['decoder_target'] = self.decoder_targets[idx]\n",
    "        \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59f0ae25-a062-4c16-a2de-3aa513f81825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, dropout=0.2):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, dropout=0.2):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, trg, hidden, cell):\n",
    "        embedded = self.dropout(self.embedding(trg))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        predictions = self.fc_out(outputs)\n",
    "        return predictions\n",
    "\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2SeqLSTM, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        hidden, cell = self.encoder(src)\n",
    "        outputs = self.decoder(trg, hidden, cell)\n",
    "        return outputs\n",
    "\n",
    "# Instantiate model components\n",
    "encoder = LSTMEncoder(input_dim=num_tokens, embedding_dim=embedding_dim, hidden_dim=hidden_size)\n",
    "decoder = LSTMDecoder(output_dim=num_tokens, embedding_dim=embedding_dim, hidden_dim=hidden_size)\n",
    "\n",
    "model = Seq2SeqLSTM(encoder, decoder).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token2idx['<pad>'])\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54c6ca8-adeb-45b8-9367-ddeb14a499f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2270/873839259.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_inputs_padded = torch.tensor(train_inputs_padded, dtype=torch.long)\n",
      "/tmp/ipykernel_2270/873839259.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_targets_padded = torch.tensor(train_targets_padded, dtype=torch.long)\n",
      "/tmp/ipykernel_2270/873839259.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_decoder_inputs = torch.tensor(train_decoder_inputs, dtype=torch.long)\n",
      "/tmp/ipykernel_2270/873839259.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_decoder_targets = torch.tensor(train_decoder_targets, dtype=torch.long)\n",
      "/tmp/ipykernel_2270/873839259.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_inputs_padded = torch.tensor(val_inputs_padded, dtype=torch.long)\n",
      "/tmp/ipykernel_2270/873839259.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_targets_padded = torch.tensor(val_targets_padded, dtype=torch.long)\n",
      "/tmp/ipykernel_2270/873839259.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_decoder_inputs = torch.tensor(val_decoder_inputs, dtype=torch.long)\n",
      "/tmp/ipykernel_2270/873839259.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_decoder_targets = torch.tensor(val_decoder_targets, dtype=torch.long)\n",
      "/tmp/ipykernel_2270/873839259.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_inputs_padded = torch.tensor(test_inputs_padded, dtype=torch.long)\n",
      "/tmp/ipykernel_2270/873839259.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_targets_padded = torch.tensor(test_targets_padded, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_inputs_padded = torch.tensor(train_inputs_padded, dtype=torch.long)\n",
    "train_targets_padded = torch.tensor(train_targets_padded, dtype=torch.long)\n",
    "train_decoder_inputs = torch.tensor(train_decoder_inputs, dtype=torch.long)\n",
    "train_decoder_targets = torch.tensor(train_decoder_targets, dtype=torch.long)\n",
    "\n",
    "val_inputs_padded = torch.tensor(val_inputs_padded, dtype=torch.long)\n",
    "val_targets_padded = torch.tensor(val_targets_padded, dtype=torch.long)\n",
    "val_decoder_inputs = torch.tensor(val_decoder_inputs, dtype=torch.long)\n",
    "val_decoder_targets = torch.tensor(val_decoder_targets, dtype=torch.long)\n",
    "\n",
    "test_inputs_padded = torch.tensor(test_inputs_padded, dtype=torch.long)\n",
    "test_targets_padded = torch.tensor(test_targets_padded, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DateTranslationDataset(\n",
    "    encoder_inputs=train_inputs_padded,\n",
    "    decoder_inputs=train_decoder_inputs,\n",
    "    decoder_targets=train_decoder_targets\n",
    ")\n",
    "\n",
    "val_dataset = DateTranslationDataset(\n",
    "    encoder_inputs=val_inputs_padded,\n",
    "    decoder_inputs=val_decoder_inputs,\n",
    "    decoder_targets=val_decoder_targets\n",
    ")\n",
    "\n",
    "test_dataset = DateTranslationDataset(\n",
    "    encoder_inputs=test_inputs_padded,\n",
    "    decoder_inputs=None,\n",
    "    decoder_targets=None\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd1ec46e-7ac5-4949-859a-6f74d8150177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 462/462 [00:02<00:00, 189.98it/s]\n",
      "100% 58/58 [00:00<00:00, 240.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss = 0.2471, Val Loss = 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 462/462 [00:02<00:00, 198.57it/s]\n",
      "100% 58/58 [00:00<00:00, 230.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Train Loss = 0.0036, Val Loss = 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 462/462 [00:02<00:00, 200.53it/s]\n",
      "100% 58/58 [00:00<00:00, 243.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: Train Loss = 0.0012, Val Loss = 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 462/462 [00:02<00:00, 190.51it/s]\n",
      "100% 58/58 [00:00<00:00, 243.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: Train Loss = 0.0006, Val Loss = 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 462/462 [00:02<00:00, 195.49it/s]\n",
      "100% 58/58 [00:00<00:00, 240.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: Train Loss = 0.0007, Val Loss = 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 462/462 [00:02<00:00, 201.19it/s]\n",
      "100% 58/58 [00:00<00:00, 237.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: Train Loss = 0.0009, Val Loss = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 462/462 [00:02<00:00, 199.05it/s]\n",
      "100% 58/58 [00:00<00:00, 258.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: Train Loss = 0.0001, Val Loss = 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 462/462 [00:02<00:00, 194.90it/s]\n",
      "100% 58/58 [00:00<00:00, 232.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: Train Loss = 0.0001, Val Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 462/462 [00:02<00:00, 196.09it/s]\n",
      "100% 58/58 [00:00<00:00, 237.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: Train Loss = 0.0001, Val Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 462/462 [00:02<00:00, 195.62it/s]\n",
      "100% 58/58 [00:00<00:00, 239.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: Train Loss = 0.0000, Val Loss = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training and validation loop\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        src = batch['encoder_input'].to(device)\n",
    "        trg = batch['decoder_input'].to(device)\n",
    "        trg_labels = batch['decoder_target'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        trg_labels = trg_labels.view(-1)\n",
    "\n",
    "        loss = criterion(output, trg_labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            src = batch['encoder_input'].to(device)\n",
    "            trg = batch['decoder_input'].to(device)\n",
    "            trg_labels = batch['decoder_target'].to(device)\n",
    "\n",
    "            output = model(src, trg)\n",
    "\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            trg_labels = trg_labels.view(-1)\n",
    "\n",
    "            loss = criterion(output, trg_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# Training loop with validation\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b365c1c-2670-4f7c-83fc-5e18cfdbed9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input date: 2025-01-05 -> Predicted date: 05-01-2025\n"
     ]
    }
   ],
   "source": [
    "# Test a specific date\n",
    "def test_specific_date(model, date, token2idx, idx2token, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode the input date\n",
    "    encoded_input = encode_dates([date], token2idx, special_tokens)\n",
    "    input_padded = pad_sequences(encoded_input, maxlen=max_input_length, padding='post', value=token2idx['<pad>'])\n",
    "    input_tensor = torch.tensor(input_padded, dtype=torch.long).to(device)\n",
    "\n",
    "    # Prepare the decoder input with the start token (<eos>)\n",
    "    start_token = token2idx['<eos>']\n",
    "    trg = torch.full((input_tensor.size(0), 1), start_token, dtype=torch.long).to(device)  # Start with <eos>\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(input_tensor)  # Get the hidden state from the encoder\n",
    "        outputs = []\n",
    "\n",
    "        for _ in range(max_target_length):  # Generate up to the maximum target length\n",
    "            output = model.decoder(trg, hidden, cell)  # Pass the target sequence and the hidden state\n",
    "            top1 = output[:, -1, :].argmax(1)  # Get the most probable next token\n",
    "            outputs.append(top1.item())  # Collect the output\n",
    "            trg = torch.cat((trg, top1.unsqueeze(1)), dim=1)  # Append predicted token to the target sequence\n",
    "\n",
    "        # Decode the output tokens back to strings\n",
    "        decoded_dates = ''.join([idx2token[idx] for idx in outputs if idx not in {token2idx['<pad>'], token2idx['<eos>']}])\n",
    "        print(f\"Input date: {date} -> Predicted date: {decoded_dates}\")\n",
    "\n",
    "# Run the test for 2025-01-05\n",
    "test_specific_date(model, '2025-01-05', token2idx, idx2token, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f014392-cfb1-493a-9c4b-1700f4abb25b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
